{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BI-LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGB61JpEWVXZQtdEd1t4r9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sLTmLZJewQEr","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","#pad_packed_sequence: 填充数据使得句子长度一致，便于LSTM进行计算\n","#pack_padded_sequence: 经过处理后将句子压紧，按列压紧，也即是去掉那些没用的占位符\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, vocab_size, emb_size, hidden_size, out_size):\n","        \"\"\"初始化参数：\n","            vocab_size:字典的大小\n","            emb_size:词向量的维数\n","            hidden_size：隐向量的维数\n","            out_size:标注的种类\n","        \"\"\"\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.bilstm = nn.LSTM(emb_size, hidden_size,\n","                              batch_first=True,\n","                              bidirectional=True)\n","\n","        self.lin = nn.Linear(2*hidden_size, out_size)\n","\n","    def forward(self, sents_tensor, lengths):\n","        emb = self.embedding(sents_tensor)  # [B, L, emb_size]\n","\n","        packed = pack_padded_sequence(emb, lengths, batch_first=True)\n","        rnn_out, _ = self.bilstm(packed)\n","        # rnn_out:[B, L, hidden_size*2]\n","        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True)\n","\n","        scores = self.lin(rnn_out)  # [B, L, out_size]\n","\n","        return scores\n","\n","    def test(self, sents_tensor, lengths, _):\n","        \"\"\"第三个参数不会用到，加它是为了与BiLSTM_CRF保持同样的接口\"\"\"\n","        logits = self.forward(sents_tensor, lengths)  # [B, L, out_size]\n","        _, batch_tagids = torch.max(logits, dim=2)\n","\n","        return batch_tagids\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vn0h1C70wuES","colab_type":"code","colab":{}},"source":["from os.path import join\n","from codecs import open\n","\n","\n","def build_corpus(split, make_vocab=True, data_dir=\"/content/gdrive/My Drive/ML/NLP/传统NLP/data\"):\n","    \"\"\"读取数据\"\"\"\n","    assert split in ['train', 'dev', 'test']\n","\n","    word_lists = []\n","    tag_lists = []\n","    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n","        word_list = []\n","        tag_list = []\n","        for line in f:\n","            if line != '\\n':\n","                word, tag = line.strip('\\n').split()\n","                word_list.append(word)\n","                tag_list.append(tag)\n","            else:\n","                word_lists.append(word_list)\n","                tag_lists.append(tag_list)\n","                word_list = []\n","                tag_list = []\n","\n","    # 如果make_vocab为True，还需要返回word2id和tag2id\n","    if make_vocab:\n","        word2id = build_map(word_lists)\n","        tag2id = build_map(tag_lists)\n","        return word_lists, tag_lists, word2id, tag2id\n","    else:\n","        return word_lists, tag_lists\n","\n","\n","def build_map(lists):\n","    maps = {}\n","    for list_ in lists:\n","        for e in list_:\n","            if e not in maps:\n","                maps[e] = len(maps)\n","\n","    return maps\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfcNfpl5xagM","colab_type":"code","colab":{}},"source":["import pickle\n","\n","\n","def merge_maps(dict1, dict2):\n","    \"\"\"用于合并两个word2id或者两个tag2id\"\"\"\n","    for key in dict2.keys():\n","        if key not in dict1:\n","            dict1[key] = len(dict1)\n","    return dict1\n","\n","\n","def save_model(model, file_name):\n","    \"\"\"用于保存模型\"\"\"\n","    with open(file_name, \"wb\") as f:\n","        pickle.dump(model, f)\n","\n","\n","def load_model(file_name):\n","    \"\"\"用于加载模型\"\"\"\n","    with open(file_name, \"rb\") as f:\n","        model = pickle.load(f)\n","    return model\n","\n","\n","# LSTM模型训练的时候需要在word2id和tag2id加入PAD和UNK\n","def extend_maps(word2id, tag2id, for_crf=True):\n","    word2id['<unk>'] = len(word2id)\n","    word2id['<pad>'] = len(word2id)\n","    tag2id['<unk>'] = len(tag2id)\n","    tag2id['<pad>'] = len(tag2id)\n","    \n","    return word2id, tag2id\n","\n","def flatten_lists(lists):\n","    flatten_list = []\n","    for l in lists:\n","        if type(l) == list:\n","            flatten_list += l\n","        else:\n","            flatten_list.append(l)\n","    return flatten_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uNXZsccxrjJ","colab_type":"code","colab":{}},"source":["from collections import Counter\n","\n","\n","class Metrics(object):\n","    \"\"\"用于评价模型，计算每个标签的精确率，召回率，F1分数\"\"\"\n","\n","    def __init__(self, golden_tags, predict_tags, remove_O=False):\n","\n","        # [[t1, t2], [t3, t4]...] --> [t1, t2, t3, t4...]\n","        self.golden_tags = flatten_lists(golden_tags)\n","        self.predict_tags = flatten_lists(predict_tags)\n","\n","        if remove_O:  # 将O标记移除，只关心实体标记\n","            self._remove_Otags()\n","\n","        # 辅助计算的变量\n","        self.tagset = set(self.golden_tags)\n","        self.correct_tags_number = self.count_correct_tags()\n","        self.predict_tags_counter = Counter(self.predict_tags)\n","        self.golden_tags_counter = Counter(self.golden_tags)\n","\n","        # 计算精确率\n","        self.precision_scores = self.cal_precision()\n","\n","        # 计算召回率\n","        self.recall_scores = self.cal_recall()\n","\n","        # 计算F1分数\n","        self.f1_scores = self.cal_f1()\n","\n","    def cal_precision(self):\n","\n","        precision_scores = {}\n","        for tag in self.tagset:\n","            precision_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n","                self.predict_tags_counter[tag]\n","\n","        return precision_scores\n","\n","    def cal_recall(self):\n","\n","        recall_scores = {}\n","        for tag in self.tagset:\n","            recall_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n","                self.golden_tags_counter[tag]\n","        return recall_scores\n","\n","    def cal_f1(self):\n","        f1_scores = {}\n","        for tag in self.tagset:\n","            p, r = self.precision_scores[tag], self.recall_scores[tag]\n","            f1_scores[tag] = 2*p*r / (p+r+1e-10)  # 加上一个特别小的数，防止分母为0\n","        return f1_scores\n","\n","    def report_scores(self):\n","        \"\"\"将结果用表格的形式打印出来，像这个样子：\n","\n","                      precision    recall  f1-score   support\n","              B-LOC      0.775     0.757     0.766      1084\n","              I-LOC      0.601     0.631     0.616       325\n","             B-MISC      0.698     0.499     0.582       339\n","             I-MISC      0.644     0.567     0.603       557\n","              B-ORG      0.795     0.801     0.798      1400\n","              I-ORG      0.831     0.773     0.801      1104\n","              B-PER      0.812     0.876     0.843       735\n","              I-PER      0.873     0.931     0.901       634\n","\n","          avg/total      0.779     0.764     0.770      6178\n","        \"\"\"\n","        # 打印表头\n","        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n","        header = ['precision', 'recall', 'f1-score', 'support']\n","        print(header_format.format('', *header))\n","\n","        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n","        # 打印每个标签的 精确率、召回率、f1分数\n","        for tag in self.tagset:\n","            print(row_format.format(\n","                tag,\n","                self.precision_scores[tag],\n","                self.recall_scores[tag],\n","                self.f1_scores[tag],\n","                self.golden_tags_counter[tag]\n","            ))\n","\n","        # 计算并打印平均值\n","        avg_metrics = self._cal_weighted_average()\n","        print(row_format.format(\n","            'avg/total',\n","            avg_metrics['precision'],\n","            avg_metrics['recall'],\n","            avg_metrics['f1_score'],\n","            len(self.golden_tags)\n","        ))\n","\n","    def count_correct_tags(self):\n","        \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n","        correct_dict = {}\n","        for gold_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n","            if gold_tag == predict_tag:\n","                if gold_tag not in correct_dict:\n","                    correct_dict[gold_tag] = 1\n","                else:\n","                    correct_dict[gold_tag] += 1\n","\n","        return correct_dict\n","\n","    def _cal_weighted_average(self):\n","\n","        weighted_average = {}\n","        total = len(self.golden_tags)\n","\n","        # 计算weighted precisions:\n","        weighted_average['precision'] = 0.\n","        weighted_average['recall'] = 0.\n","        weighted_average['f1_score'] = 0.\n","        for tag in self.tagset:\n","            size = self.golden_tags_counter[tag]\n","            weighted_average['precision'] += self.precision_scores[tag] * size\n","            weighted_average['recall'] += self.recall_scores[tag] * size\n","            weighted_average['f1_score'] += self.f1_scores[tag] * size\n","\n","        for metric in weighted_average.keys():\n","            weighted_average[metric] /= total\n","\n","        return weighted_average\n","\n","    def _remove_Otags(self):\n","\n","        length = len(self.golden_tags)\n","        O_tag_indices = [i for i in range(length)\n","                         if self.golden_tags[i] == 'O']\n","\n","        self.golden_tags = [tag for i, tag in enumerate(self.golden_tags)\n","                            if i not in O_tag_indices]\n","\n","        self.predict_tags = [tag for i, tag in enumerate(self.predict_tags)\n","                             if i not in O_tag_indices]\n","        print(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n","            length,\n","            len(O_tag_indices),\n","            len(O_tag_indices) / length * 100\n","        ))\n","\n","    def report_confusion_matrix(self):\n","        \"\"\"计算混淆矩阵\"\"\"\n","\n","        print(\"\\nConfusion Matrix:\")\n","        tag_list = list(self.tagset)\n","        # 初始化混淆矩阵 matrix[i][j]表示第i个tag被模型预测成第j个tag的次数\n","        tags_size = len(tag_list)\n","        matrix = []\n","        for i in range(tags_size):\n","            matrix.append([0] * tags_size)\n","\n","        # 遍历tags列表\n","        for golden_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n","            try:\n","                row = tag_list.index(golden_tag)\n","                col = tag_list.index(predict_tag)\n","                matrix[row][col] += 1\n","            except ValueError:  # 有极少数标记没有出现在golden_tags，但出现在predict_tags，跳过这些标记\n","                continue\n","\n","        # 输出矩阵\n","        row_format_ = '{:>7} ' * (tags_size+1)\n","        print(row_format_.format(\"\", *tag_list))\n","        for i, row in enumerate(matrix):\n","            print(row_format_.format(tag_list[i], *row))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoZU4Gs_YUlP","colab_type":"code","colab":{}},"source":["# 设置lstm训练参数\n","class TrainingConfig(object):\n","    batch_size = 64\n","    # 学习速率\n","    lr = 0.001\n","    epoches = 30\n","    print_step = 5\n","\n","\n","class LSTMConfig(object):\n","    emb_size = 128  # 词向量的维数\n","    hidden_size = 128  # lstm隐向量的维数\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AnKOjwrZIrb","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","\n","# ******** LSTM模型 工具函数*************\n","\n","def tensorized(batch, maps):\n","    PAD = maps.get('<pad>')\n","    UNK = maps.get('<unk>')\n","\n","    max_len = len(batch[0])\n","    batch_size = len(batch)\n","\n","    batch_tensor = torch.ones(batch_size, max_len).long() * PAD\n","    for i, l in enumerate(batch):\n","        for j, e in enumerate(l):\n","            batch_tensor[i][j] = maps.get(e, UNK)\n","    # batch各个元素的长度\n","    lengths = [len(l) for l in batch]\n","\n","    return batch_tensor, lengths\n","\n","\n","def sort_by_lengths(word_lists, tag_lists):\n","    pairs = list(zip(word_lists, tag_lists))\n","    indices = sorted(range(len(pairs)),\n","                     key=lambda k: len(pairs[k][0]),\n","                     reverse=True)\n","    pairs = [pairs[i] for i in indices]\n","    # pairs.sort(key=lambda pair: len(pair[0]), reverse=True)\n","\n","    word_lists, tag_lists = list(zip(*pairs))\n","\n","    return word_lists, tag_lists, indices\n","\n","\n","def cal_loss(logits, targets, tag2id):\n","    \"\"\"计算损失\n","    参数:\n","        logits: [B, L, out_size]\n","        targets: [B, L]\n","        lengths: [B]\n","    \"\"\"\n","    PAD = tag2id.get('<pad>')\n","    assert PAD is not None\n","\n","    mask = (targets != PAD)  # [B, L]\n","    targets = targets[mask]\n","    out_size = logits.size(2)\n","    logits = logits.masked_select(\n","        mask.unsqueeze(2).expand(-1, -1, out_size)\n","    ).contiguous().view(-1, out_size)\n","\n","    assert logits.size(0) == targets.size(0)\n","    loss = F.cross_entropy(logits, targets)\n","\n","    return loss\n","\n","\n","\n","def indexed(targets, tagset_size, start_id):\n","    \"\"\"将targets中的数转化为在[T*T]大小序列中的索引,T是标注的种类\"\"\"\n","    batch_size, max_len = targets.size()\n","    for col in range(max_len-1, 0, -1):\n","        targets[:, col] += (targets[:, col-1] * tagset_size)\n","    targets[:, 0] += (start_id * tagset_size)\n","    return targets\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"92KnsPuqYZ7j","colab_type":"code","colab":{}},"source":["from itertools import zip_longest\n","from copy import deepcopy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class BILSTM_Model(object):\n","    def __init__(self, vocab_size, out_size, crf=True):\n","        \"\"\"功能：对LSTM的模型进行训练与测试\n","           参数:\n","            vocab_size:词典大小\n","            out_size:标注种类\n","            crf选择是否添加CRF层\"\"\"\n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # 加载模型参数\n","        self.emb_size = LSTMConfig.emb_size\n","        self.hidden_size = LSTMConfig.hidden_size\n","\n","        self.crf = crf\n","        # 根据是否添加crf初始化不同的模型 选择不一样的损失计算函数\n","        if not crf:\n","            self.model = BiLSTM(vocab_size, self.emb_size,\n","                                self.hidden_size, out_size).to(self.device)\n","            self.cal_loss_func = cal_loss\n","        else:\n","            self.model = BiLSTM_CRF(vocab_size, self.emb_size,\n","                                    self.hidden_size, out_size).to(self.device)\n","            self.cal_loss_func = cal_lstm_crf_loss\n","\n","        # 加载训练参数：\n","        self.epoches = TrainingConfig.epoches\n","        self.print_step = TrainingConfig.print_step\n","        self.lr = TrainingConfig.lr\n","        self.batch_size = TrainingConfig.batch_size\n","\n","        # 初始化优化器\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n","\n","        # 初始化其他指标\n","        self.step = 0\n","        self._best_val_loss = 1e18\n","        self.best_model = None\n","\n","    def train(self, word_lists, tag_lists,\n","              dev_word_lists, dev_tag_lists,\n","              word2id, tag2id):\n","        # 对数据集按照长度进行排序\n","        word_lists, tag_lists, _ = sort_by_lengths(word_lists, tag_lists)\n","        dev_word_lists, dev_tag_lists, _ = sort_by_lengths(\n","            dev_word_lists, dev_tag_lists)\n","\n","        B = self.batch_size\n","        for e in range(1, self.epoches+1):\n","            self.step = 0\n","            losses = 0.\n","            for ind in range(0, len(word_lists), B):\n","                batch_sents = word_lists[ind:ind+B]\n","                batch_tags = tag_lists[ind:ind+B]\n","\n","                losses += self.train_step(batch_sents, batch_tags, word2id, tag2id)\n","\n","                if self.step % TrainingConfig.print_step == 0:\n","                    total_step = (len(word_lists) // B + 1)\n","                    print(\"Epoch {}, step/total_step: {}/{} {:.2f}% Loss:{:.4f}\".format(\n","                        e, self.step, total_step,\n","                        100. * self.step / total_step,\n","                        losses / self.print_step\n","                    ))\n","                    losses = 0.\n","\n","            # 每轮结束测试在验证集上的性能，保存最好的一个\n","            val_loss = self.validate(\n","                dev_word_lists, dev_tag_lists, word2id, tag2id)\n","            print(\"Epoch {}, Val Loss:{:.4f}\".format(e, val_loss))\n","\n","    def train_step(self, batch_sents, batch_tags, word2id, tag2id):\n","        self.model.train()\n","        self.step += 1\n","        # 准备数据\n","        tensorized_sents, lengths = tensorized(batch_sents, word2id)\n","        tensorized_sents = tensorized_sents.to(self.device)\n","        targets, lengths = tensorized(batch_tags, tag2id)\n","        targets = targets.to(self.device)\n","\n","        # forward\n","        scores = self.model(tensorized_sents, lengths)\n","\n","        # 计算损失 更新参数\n","        self.optimizer.zero_grad()\n","        loss = self.cal_loss_func(scores, targets, tag2id).to(self.device)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    def validate(self, dev_word_lists, dev_tag_lists, word2id, tag2id):\n","        self.model.eval()\n","        with torch.no_grad():\n","            val_losses = 0.\n","            val_step = 0\n","            for ind in range(0, len(dev_word_lists), self.batch_size):\n","                val_step += 1\n","                # 准备batch数据\n","                batch_sents = dev_word_lists[ind:ind+self.batch_size]\n","                batch_tags = dev_tag_lists[ind:ind+self.batch_size]\n","                tensorized_sents, lengths = tensorized(\n","                    batch_sents, word2id)\n","                tensorized_sents = tensorized_sents.to(self.device)\n","                targets, lengths = tensorized(batch_tags, tag2id)\n","                targets = targets.to(self.device)\n","\n","                # forward\n","                scores = self.model(tensorized_sents, lengths)\n","\n","                # 计算损失\n","                loss = self.cal_loss_func(\n","                    scores, targets, tag2id).to(self.device)\n","                val_losses += loss.item()\n","            val_loss = val_losses / val_step\n","\n","            if val_loss < self._best_val_loss:\n","                print(\"保存模型...\")\n","                self.best_model = deepcopy(self.model)\n","                self._best_val_loss = val_loss\n","\n","            return val_loss\n","\n","    def test(self, word_lists, tag_lists, word2id, tag2id):\n","        \"\"\"返回最佳模型在测试集上的预测结果\"\"\"\n","        # 准备数据\n","        word_lists, tag_lists, indices = sort_by_lengths(word_lists, tag_lists)\n","        tensorized_sents, lengths = tensorized(word_lists, word2id)\n","        tensorized_sents = tensorized_sents.to(self.device)\n","\n","        self.best_model.eval()\n","        with torch.no_grad():\n","            batch_tagids = self.best_model.test(\n","                tensorized_sents, lengths, tag2id)\n","\n","        # 将id转化为标注\n","        pred_tag_lists = []\n","        id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\n","        for i, ids in enumerate(batch_tagids):\n","            tag_list = []\n","            if self.crf:\n","                for j in range(lengths[i] - 1):  # crf解码过程中，end被舍弃\n","                    tag_list.append(id2tag[ids[j].item()])\n","            else:\n","                for j in range(lengths[i]):\n","                    tag_list.append(id2tag[ids[j].item()])\n","            pred_tag_lists.append(tag_list)\n","\n","        # indices存有根据长度排序后的索引映射的信息\n","        # 比如若indices = [1, 2, 0] 则说明原先索引为1的元素映射到的新的索引是0，\n","        # 索引为2的元素映射到新的索引是1...\n","        # 下面根据indices将pred_tag_lists和tag_lists转化为原来的顺序\n","        ind_maps = sorted(list(enumerate(indices)), key=lambda e: e[1])\n","        indices, _ = list(zip(*ind_maps))\n","        pred_tag_lists = [pred_tag_lists[i] for i in indices]\n","        tag_lists = [tag_lists[i] for i in indices]\n","\n","        return pred_tag_lists, tag_lists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvzzxAqjxtzW","colab_type":"code","colab":{}},"source":["import time\n","from collections import Counter\n","def bilstm_train_and_eval(train_data, dev_data, test_data,\n","                          word2id, tag2id, crf=True, remove_O=False):\n","    train_word_lists, train_tag_lists = train_data\n","    dev_word_lists, dev_tag_lists = dev_data\n","    test_word_lists, test_tag_lists = test_data\n","\n","    start = time.time()\n","    vocab_size = len(word2id)\n","    out_size = len(tag2id)\n","    bilstm_model = BILSTM_Model(vocab_size, out_size, crf=crf)\n","    bilstm_model.train(train_word_lists, train_tag_lists,\n","                       dev_word_lists, dev_tag_lists, word2id, tag2id)\n","\n","    model_name = \"bilstm_crf\" if crf else \"bilstm\"\n","    save_model(bilstm_model, \"/content/gdrive/My Drive/ML/NLP/传统NLP/data/\"+model_name+\".pkl\")\n","\n","    print(\"训练完毕,共用时{}秒.\".format(int(time.time()-start)))\n","    print(\"评估{}模型中...\".format(model_name))\n","    pred_tag_lists, test_tag_lists = bilstm_model.test(\n","        test_word_lists, test_tag_lists, word2id, tag2id)\n","\n","    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=remove_O)\n","    metrics.report_scores()\n","    metrics.report_confusion_matrix()\n","\n","    return pred_tag_lists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_y4u5JJ31Q2u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d6d91aad-ad00-4eb9-e98a-3314983dbff6","executionInfo":{"status":"ok","timestamp":1585471112320,"user_tz":-480,"elapsed":36955,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rrNed0p-1YD8","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"09fZ3PdV1I0L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"23260cc4-8bca-44cc-a307-ce4a0ab5c907","executionInfo":{"status":"ok","timestamp":1585471223041,"user_tz":-480,"elapsed":147618,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}}},"source":["def main():\n","    \"\"\"训练模型，评估结果\"\"\"\n","\n","    # 读取数据\n","    print(\"读取数据...\")\n","    train_word_lists, train_tag_lists, word2id, tag2id = \\\n","        build_corpus(\"train\")\n","    dev_word_lists, dev_tag_lists = build_corpus(\"dev\", make_vocab=False)\n","    test_word_lists, test_tag_lists = build_corpus(\"test\", make_vocab=False)\n","\n","    # 训练评估BI-LSTM模型\n","    print(\"正在训练评估双向LSTM模型...\")\n","    # LSTM模型训练的时候需要在word2id和tag2id加入PAD和UNK\n","    bilstm_word2id, bilstm_tag2id = extend_maps(word2id, tag2id, for_crf=False)\n","    lstm_pred = bilstm_train_and_eval(\n","        (train_word_lists, train_tag_lists),\n","        (dev_word_lists, dev_tag_lists),\n","        (test_word_lists, test_tag_lists),\n","        bilstm_word2id, bilstm_tag2id,\n","        crf=False\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"execution_count":40,"outputs":[{"output_type":"stream","text":["读取数据...\n","正在训练评估双向LSTM模型...\n","Epoch 1, step/total_step: 5/60 8.33% Loss:3.3105\n","Epoch 1, step/total_step: 10/60 16.67% Loss:3.0112\n","Epoch 1, step/total_step: 15/60 25.00% Loss:2.5431\n","Epoch 1, step/total_step: 20/60 33.33% Loss:1.6951\n","Epoch 1, step/total_step: 25/60 41.67% Loss:1.3390\n","Epoch 1, step/total_step: 30/60 50.00% Loss:1.1847\n","Epoch 1, step/total_step: 35/60 58.33% Loss:1.0674\n","Epoch 1, step/total_step: 40/60 66.67% Loss:1.0771\n","Epoch 1, step/total_step: 45/60 75.00% Loss:1.0331\n","Epoch 1, step/total_step: 50/60 83.33% Loss:0.9923\n","Epoch 1, step/total_step: 55/60 91.67% Loss:1.4856\n","Epoch 1, step/total_step: 60/60 100.00% Loss:1.4995\n","保存模型...\n","Epoch 1, Val Loss:0.9088\n","Epoch 2, step/total_step: 5/60 8.33% Loss:1.0621\n","Epoch 2, step/total_step: 10/60 16.67% Loss:0.8556\n","Epoch 2, step/total_step: 15/60 25.00% Loss:0.7106\n","Epoch 2, step/total_step: 20/60 33.33% Loss:0.5801\n","Epoch 2, step/total_step: 25/60 41.67% Loss:0.5534\n","Epoch 2, step/total_step: 30/60 50.00% Loss:0.4879\n","Epoch 2, step/total_step: 35/60 58.33% Loss:0.4674\n","Epoch 2, step/total_step: 40/60 66.67% Loss:0.4848\n","Epoch 2, step/total_step: 45/60 75.00% Loss:0.4772\n","Epoch 2, step/total_step: 50/60 83.33% Loss:0.4861\n","Epoch 2, step/total_step: 55/60 91.67% Loss:0.7575\n","Epoch 2, step/total_step: 60/60 100.00% Loss:0.7554\n","保存模型...\n","Epoch 2, Val Loss:0.4655\n","Epoch 3, step/total_step: 5/60 8.33% Loss:0.4990\n","Epoch 3, step/total_step: 10/60 16.67% Loss:0.4865\n","Epoch 3, step/total_step: 15/60 25.00% Loss:0.3962\n","Epoch 3, step/total_step: 20/60 33.33% Loss:0.3231\n","Epoch 3, step/total_step: 25/60 41.67% Loss:0.3171\n","Epoch 3, step/total_step: 30/60 50.00% Loss:0.2722\n","Epoch 3, step/total_step: 35/60 58.33% Loss:0.2785\n","Epoch 3, step/total_step: 40/60 66.67% Loss:0.2860\n","Epoch 3, step/total_step: 45/60 75.00% Loss:0.2816\n","Epoch 3, step/total_step: 50/60 83.33% Loss:0.2890\n","Epoch 3, step/total_step: 55/60 91.67% Loss:0.3874\n","Epoch 3, step/total_step: 60/60 100.00% Loss:0.4154\n","保存模型...\n","Epoch 3, Val Loss:0.3142\n","Epoch 4, step/total_step: 5/60 8.33% Loss:0.3512\n","Epoch 4, step/total_step: 10/60 16.67% Loss:0.3385\n","Epoch 4, step/total_step: 15/60 25.00% Loss:0.2709\n","Epoch 4, step/total_step: 20/60 33.33% Loss:0.2142\n","Epoch 4, step/total_step: 25/60 41.67% Loss:0.2166\n","Epoch 4, step/total_step: 30/60 50.00% Loss:0.1802\n","Epoch 4, step/total_step: 35/60 58.33% Loss:0.1929\n","Epoch 4, step/total_step: 40/60 66.67% Loss:0.1967\n","Epoch 4, step/total_step: 45/60 75.00% Loss:0.1931\n","Epoch 4, step/total_step: 50/60 83.33% Loss:0.1970\n","Epoch 4, step/total_step: 55/60 91.67% Loss:0.2442\n","Epoch 4, step/total_step: 60/60 100.00% Loss:0.2686\n","保存模型...\n","Epoch 4, Val Loss:0.2464\n","Epoch 5, step/total_step: 5/60 8.33% Loss:0.2729\n","Epoch 5, step/total_step: 10/60 16.67% Loss:0.2546\n","Epoch 5, step/total_step: 15/60 25.00% Loss:0.2041\n","Epoch 5, step/total_step: 20/60 33.33% Loss:0.1575\n","Epoch 5, step/total_step: 25/60 41.67% Loss:0.1610\n","Epoch 5, step/total_step: 30/60 50.00% Loss:0.1314\n","Epoch 5, step/total_step: 35/60 58.33% Loss:0.1436\n","Epoch 5, step/total_step: 40/60 66.67% Loss:0.1414\n","Epoch 5, step/total_step: 45/60 75.00% Loss:0.1418\n","Epoch 5, step/total_step: 50/60 83.33% Loss:0.1432\n","Epoch 5, step/total_step: 55/60 91.67% Loss:0.1692\n","Epoch 5, step/total_step: 60/60 100.00% Loss:0.1833\n","保存模型...\n","Epoch 5, Val Loss:0.2099\n","Epoch 6, step/total_step: 5/60 8.33% Loss:0.2285\n","Epoch 6, step/total_step: 10/60 16.67% Loss:0.2046\n","Epoch 6, step/total_step: 15/60 25.00% Loss:0.1642\n","Epoch 6, step/total_step: 20/60 33.33% Loss:0.1211\n","Epoch 6, step/total_step: 25/60 41.67% Loss:0.1256\n","Epoch 6, step/total_step: 30/60 50.00% Loss:0.1015\n","Epoch 6, step/total_step: 35/60 58.33% Loss:0.1121\n","Epoch 6, step/total_step: 40/60 66.67% Loss:0.1071\n","Epoch 6, step/total_step: 45/60 75.00% Loss:0.1074\n","Epoch 6, step/total_step: 50/60 83.33% Loss:0.1089\n","Epoch 6, step/total_step: 55/60 91.67% Loss:0.1212\n","Epoch 6, step/total_step: 60/60 100.00% Loss:0.1268\n","保存模型...\n","Epoch 6, Val Loss:0.1891\n","Epoch 7, step/total_step: 5/60 8.33% Loss:0.1994\n","Epoch 7, step/total_step: 10/60 16.67% Loss:0.1693\n","Epoch 7, step/total_step: 15/60 25.00% Loss:0.1347\n","Epoch 7, step/total_step: 20/60 33.33% Loss:0.0959\n","Epoch 7, step/total_step: 25/60 41.67% Loss:0.1038\n","Epoch 7, step/total_step: 30/60 50.00% Loss:0.0814\n","Epoch 7, step/total_step: 35/60 58.33% Loss:0.0905\n","Epoch 7, step/total_step: 40/60 66.67% Loss:0.0845\n","Epoch 7, step/total_step: 45/60 75.00% Loss:0.0840\n","Epoch 7, step/total_step: 50/60 83.33% Loss:0.0851\n","Epoch 7, step/total_step: 55/60 91.67% Loss:0.0901\n","Epoch 7, step/total_step: 60/60 100.00% Loss:0.0891\n","保存模型...\n","Epoch 7, Val Loss:0.1781\n","Epoch 8, step/total_step: 5/60 8.33% Loss:0.1773\n","Epoch 8, step/total_step: 10/60 16.67% Loss:0.1429\n","Epoch 8, step/total_step: 15/60 25.00% Loss:0.1127\n","Epoch 8, step/total_step: 20/60 33.33% Loss:0.0775\n","Epoch 8, step/total_step: 25/60 41.67% Loss:0.0866\n","Epoch 8, step/total_step: 30/60 50.00% Loss:0.0668\n","Epoch 8, step/total_step: 35/60 58.33% Loss:0.0756\n","Epoch 8, step/total_step: 40/60 66.67% Loss:0.0666\n","Epoch 8, step/total_step: 45/60 75.00% Loss:0.0659\n","Epoch 8, step/total_step: 50/60 83.33% Loss:0.0675\n","Epoch 8, step/total_step: 55/60 91.67% Loss:0.0680\n","Epoch 8, step/total_step: 60/60 100.00% Loss:0.0634\n","保存模型...\n","Epoch 8, Val Loss:0.1707\n","Epoch 9, step/total_step: 5/60 8.33% Loss:0.1580\n","Epoch 9, step/total_step: 10/60 16.67% Loss:0.1210\n","Epoch 9, step/total_step: 15/60 25.00% Loss:0.0948\n","Epoch 9, step/total_step: 20/60 33.33% Loss:0.0630\n","Epoch 9, step/total_step: 25/60 41.67% Loss:0.0733\n","Epoch 9, step/total_step: 30/60 50.00% Loss:0.0555\n","Epoch 9, step/total_step: 35/60 58.33% Loss:0.0634\n","Epoch 9, step/total_step: 40/60 66.67% Loss:0.0533\n","Epoch 9, step/total_step: 45/60 75.00% Loss:0.0518\n","Epoch 9, step/total_step: 50/60 83.33% Loss:0.0540\n","Epoch 9, step/total_step: 55/60 91.67% Loss:0.0525\n","Epoch 9, step/total_step: 60/60 100.00% Loss:0.0459\n","保存模型...\n","Epoch 9, Val Loss:0.1652\n","Epoch 10, step/total_step: 5/60 8.33% Loss:0.1397\n","Epoch 10, step/total_step: 10/60 16.67% Loss:0.1023\n","Epoch 10, step/total_step: 15/60 25.00% Loss:0.0811\n","Epoch 10, step/total_step: 20/60 33.33% Loss:0.0511\n","Epoch 10, step/total_step: 25/60 41.67% Loss:0.0631\n","Epoch 10, step/total_step: 30/60 50.00% Loss:0.0461\n","Epoch 10, step/total_step: 35/60 58.33% Loss:0.0531\n","Epoch 10, step/total_step: 40/60 66.67% Loss:0.0430\n","Epoch 10, step/total_step: 45/60 75.00% Loss:0.0407\n","Epoch 10, step/total_step: 50/60 83.33% Loss:0.0439\n","Epoch 10, step/total_step: 55/60 91.67% Loss:0.0415\n","Epoch 10, step/total_step: 60/60 100.00% Loss:0.0340\n","保存模型...\n","Epoch 10, Val Loss:0.1573\n","Epoch 11, step/total_step: 5/60 8.33% Loss:0.1219\n","Epoch 11, step/total_step: 10/60 16.67% Loss:0.0867\n","Epoch 11, step/total_step: 15/60 25.00% Loss:0.0707\n","Epoch 11, step/total_step: 20/60 33.33% Loss:0.0421\n","Epoch 11, step/total_step: 25/60 41.67% Loss:0.0541\n","Epoch 11, step/total_step: 30/60 50.00% Loss:0.0386\n","Epoch 11, step/total_step: 35/60 58.33% Loss:0.0438\n","Epoch 11, step/total_step: 40/60 66.67% Loss:0.0355\n","Epoch 11, step/total_step: 45/60 75.00% Loss:0.0327\n","Epoch 11, step/total_step: 50/60 83.33% Loss:0.0359\n","Epoch 11, step/total_step: 55/60 91.67% Loss:0.0337\n","Epoch 11, step/total_step: 60/60 100.00% Loss:0.0256\n","保存模型...\n","Epoch 11, Val Loss:0.1526\n","Epoch 12, step/total_step: 5/60 8.33% Loss:0.1048\n","Epoch 12, step/total_step: 10/60 16.67% Loss:0.0746\n","Epoch 12, step/total_step: 15/60 25.00% Loss:0.0614\n","Epoch 12, step/total_step: 20/60 33.33% Loss:0.0355\n","Epoch 12, step/total_step: 25/60 41.67% Loss:0.0470\n","Epoch 12, step/total_step: 30/60 50.00% Loss:0.0333\n","Epoch 12, step/total_step: 35/60 58.33% Loss:0.0368\n","Epoch 12, step/total_step: 40/60 66.67% Loss:0.0307\n","Epoch 12, step/total_step: 45/60 75.00% Loss:0.0265\n","Epoch 12, step/total_step: 50/60 83.33% Loss:0.0296\n","Epoch 12, step/total_step: 55/60 91.67% Loss:0.0273\n","Epoch 12, step/total_step: 60/60 100.00% Loss:0.0198\n","保存模型...\n","Epoch 12, Val Loss:0.1451\n","Epoch 13, step/total_step: 5/60 8.33% Loss:0.0922\n","Epoch 13, step/total_step: 10/60 16.67% Loss:0.0631\n","Epoch 13, step/total_step: 15/60 25.00% Loss:0.0534\n","Epoch 13, step/total_step: 20/60 33.33% Loss:0.0310\n","Epoch 13, step/total_step: 25/60 41.67% Loss:0.0400\n","Epoch 13, step/total_step: 30/60 50.00% Loss:0.0288\n","Epoch 13, step/total_step: 35/60 58.33% Loss:0.0345\n","Epoch 13, step/total_step: 40/60 66.67% Loss:0.0273\n","Epoch 13, step/total_step: 45/60 75.00% Loss:0.0228\n","Epoch 13, step/total_step: 50/60 83.33% Loss:0.0254\n","Epoch 13, step/total_step: 55/60 91.67% Loss:0.0227\n","Epoch 13, step/total_step: 60/60 100.00% Loss:0.0156\n","Epoch 13, Val Loss:0.1465\n","Epoch 14, step/total_step: 5/60 8.33% Loss:0.0835\n","Epoch 14, step/total_step: 10/60 16.67% Loss:0.0559\n","Epoch 14, step/total_step: 15/60 25.00% Loss:0.0476\n","Epoch 14, step/total_step: 20/60 33.33% Loss:0.0259\n","Epoch 14, step/total_step: 25/60 41.67% Loss:0.0328\n","Epoch 14, step/total_step: 30/60 50.00% Loss:0.0243\n","Epoch 14, step/total_step: 35/60 58.33% Loss:0.0279\n","Epoch 14, step/total_step: 40/60 66.67% Loss:0.0246\n","Epoch 14, step/total_step: 45/60 75.00% Loss:0.0194\n","Epoch 14, step/total_step: 50/60 83.33% Loss:0.0205\n","Epoch 14, step/total_step: 55/60 91.67% Loss:0.0195\n","Epoch 14, step/total_step: 60/60 100.00% Loss:0.0129\n","Epoch 14, Val Loss:0.1487\n","Epoch 15, step/total_step: 5/60 8.33% Loss:0.0757\n","Epoch 15, step/total_step: 10/60 16.67% Loss:0.0516\n","Epoch 15, step/total_step: 15/60 25.00% Loss:0.0418\n","Epoch 15, step/total_step: 20/60 33.33% Loss:0.0239\n","Epoch 15, step/total_step: 25/60 41.67% Loss:0.0276\n","Epoch 15, step/total_step: 30/60 50.00% Loss:0.0208\n","Epoch 15, step/total_step: 35/60 58.33% Loss:0.0258\n","Epoch 15, step/total_step: 40/60 66.67% Loss:0.0202\n","Epoch 15, step/total_step: 45/60 75.00% Loss:0.0183\n","Epoch 15, step/total_step: 50/60 83.33% Loss:0.0168\n","Epoch 15, step/total_step: 55/60 91.67% Loss:0.0158\n","Epoch 15, step/total_step: 60/60 100.00% Loss:0.0107\n","Epoch 15, Val Loss:0.1570\n","Epoch 16, step/total_step: 5/60 8.33% Loss:0.0718\n","Epoch 16, step/total_step: 10/60 16.67% Loss:0.0476\n","Epoch 16, step/total_step: 15/60 25.00% Loss:0.0432\n","Epoch 16, step/total_step: 20/60 33.33% Loss:0.0268\n","Epoch 16, step/total_step: 25/60 41.67% Loss:0.0344\n","Epoch 16, step/total_step: 30/60 50.00% Loss:0.0217\n","Epoch 16, step/total_step: 35/60 58.33% Loss:0.0223\n","Epoch 16, step/total_step: 40/60 66.67% Loss:0.0189\n","Epoch 16, step/total_step: 45/60 75.00% Loss:0.0159\n","Epoch 16, step/total_step: 50/60 83.33% Loss:0.0164\n","Epoch 16, step/total_step: 55/60 91.67% Loss:0.0135\n","Epoch 16, step/total_step: 60/60 100.00% Loss:0.0091\n","Epoch 16, Val Loss:0.1501\n","Epoch 17, step/total_step: 5/60 8.33% Loss:0.0605\n","Epoch 17, step/total_step: 10/60 16.67% Loss:0.0407\n","Epoch 17, step/total_step: 15/60 25.00% Loss:0.0416\n","Epoch 17, step/total_step: 20/60 33.33% Loss:0.0202\n","Epoch 17, step/total_step: 25/60 41.67% Loss:0.0276\n","Epoch 17, step/total_step: 30/60 50.00% Loss:0.0197\n","Epoch 17, step/total_step: 35/60 58.33% Loss:0.0253\n","Epoch 17, step/total_step: 40/60 66.67% Loss:0.0194\n","Epoch 17, step/total_step: 45/60 75.00% Loss:0.0163\n","Epoch 17, step/total_step: 50/60 83.33% Loss:0.0146\n","Epoch 17, step/total_step: 55/60 91.67% Loss:0.0120\n","Epoch 17, step/total_step: 60/60 100.00% Loss:0.0078\n","Epoch 17, Val Loss:0.1626\n","Epoch 18, step/total_step: 5/60 8.33% Loss:0.0686\n","Epoch 18, step/total_step: 10/60 16.67% Loss:0.0403\n","Epoch 18, step/total_step: 15/60 25.00% Loss:0.0336\n","Epoch 18, step/total_step: 20/60 33.33% Loss:0.0214\n","Epoch 18, step/total_step: 25/60 41.67% Loss:0.0208\n","Epoch 18, step/total_step: 30/60 50.00% Loss:0.0266\n","Epoch 18, step/total_step: 35/60 58.33% Loss:0.0189\n","Epoch 18, step/total_step: 40/60 66.67% Loss:0.0236\n","Epoch 18, step/total_step: 45/60 75.00% Loss:0.0128\n","Epoch 18, step/total_step: 50/60 83.33% Loss:0.0136\n","Epoch 18, step/total_step: 55/60 91.67% Loss:0.0112\n","Epoch 18, step/total_step: 60/60 100.00% Loss:0.0068\n","Epoch 18, Val Loss:0.1505\n","Epoch 19, step/total_step: 5/60 8.33% Loss:0.0602\n","Epoch 19, step/total_step: 10/60 16.67% Loss:0.0380\n","Epoch 19, step/total_step: 15/60 25.00% Loss:0.0306\n","Epoch 19, step/total_step: 20/60 33.33% Loss:0.0171\n","Epoch 19, step/total_step: 25/60 41.67% Loss:0.0186\n","Epoch 19, step/total_step: 30/60 50.00% Loss:0.0136\n","Epoch 19, step/total_step: 35/60 58.33% Loss:0.0171\n","Epoch 19, step/total_step: 40/60 66.67% Loss:0.0121\n","Epoch 19, step/total_step: 45/60 75.00% Loss:0.0096\n","Epoch 19, step/total_step: 50/60 83.33% Loss:0.0098\n","Epoch 19, step/total_step: 55/60 91.67% Loss:0.0092\n","Epoch 19, step/total_step: 60/60 100.00% Loss:0.0056\n","保存模型...\n","Epoch 19, Val Loss:0.1418\n","Epoch 20, step/total_step: 5/60 8.33% Loss:0.0454\n","Epoch 20, step/total_step: 10/60 16.67% Loss:0.0295\n","Epoch 20, step/total_step: 15/60 25.00% Loss:0.0269\n","Epoch 20, step/total_step: 20/60 33.33% Loss:0.0125\n","Epoch 20, step/total_step: 25/60 41.67% Loss:0.0165\n","Epoch 20, step/total_step: 30/60 50.00% Loss:0.0105\n","Epoch 20, step/total_step: 35/60 58.33% Loss:0.0124\n","Epoch 20, step/total_step: 40/60 66.67% Loss:0.0114\n","Epoch 20, step/total_step: 45/60 75.00% Loss:0.0074\n","Epoch 20, step/total_step: 50/60 83.33% Loss:0.0081\n","Epoch 20, step/total_step: 55/60 91.67% Loss:0.0075\n","Epoch 20, step/total_step: 60/60 100.00% Loss:0.0049\n","Epoch 20, Val Loss:0.1446\n","Epoch 21, step/total_step: 5/60 8.33% Loss:0.0374\n","Epoch 21, step/total_step: 10/60 16.67% Loss:0.0233\n","Epoch 21, step/total_step: 15/60 25.00% Loss:0.0221\n","Epoch 21, step/total_step: 20/60 33.33% Loss:0.0120\n","Epoch 21, step/total_step: 25/60 41.67% Loss:0.0113\n","Epoch 21, step/total_step: 30/60 50.00% Loss:0.0104\n","Epoch 21, step/total_step: 35/60 58.33% Loss:0.0102\n","Epoch 21, step/total_step: 40/60 66.67% Loss:0.0087\n","Epoch 21, step/total_step: 45/60 75.00% Loss:0.0066\n","Epoch 21, step/total_step: 50/60 83.33% Loss:0.0072\n","Epoch 21, step/total_step: 55/60 91.67% Loss:0.0066\n","Epoch 21, step/total_step: 60/60 100.00% Loss:0.0043\n","Epoch 21, Val Loss:0.1464\n","Epoch 22, step/total_step: 5/60 8.33% Loss:0.0308\n","Epoch 22, step/total_step: 10/60 16.67% Loss:0.0202\n","Epoch 22, step/total_step: 15/60 25.00% Loss:0.0172\n","Epoch 22, step/total_step: 20/60 33.33% Loss:0.0113\n","Epoch 22, step/total_step: 25/60 41.67% Loss:0.0104\n","Epoch 22, step/total_step: 30/60 50.00% Loss:0.0072\n","Epoch 22, step/total_step: 35/60 58.33% Loss:0.0102\n","Epoch 22, step/total_step: 40/60 66.67% Loss:0.0085\n","Epoch 22, step/total_step: 45/60 75.00% Loss:0.0055\n","Epoch 22, step/total_step: 50/60 83.33% Loss:0.0064\n","Epoch 22, step/total_step: 55/60 91.67% Loss:0.0059\n","Epoch 22, step/total_step: 60/60 100.00% Loss:0.0038\n","Epoch 22, Val Loss:0.1522\n","Epoch 23, step/total_step: 5/60 8.33% Loss:0.0268\n","Epoch 23, step/total_step: 10/60 16.67% Loss:0.0170\n","Epoch 23, step/total_step: 15/60 25.00% Loss:0.0149\n","Epoch 23, step/total_step: 20/60 33.33% Loss:0.0075\n","Epoch 23, step/total_step: 25/60 41.67% Loss:0.0103\n","Epoch 23, step/total_step: 30/60 50.00% Loss:0.0063\n","Epoch 23, step/total_step: 35/60 58.33% Loss:0.0074\n","Epoch 23, step/total_step: 40/60 66.67% Loss:0.0076\n","Epoch 23, step/total_step: 45/60 75.00% Loss:0.0067\n","Epoch 23, step/total_step: 50/60 83.33% Loss:0.0055\n","Epoch 23, step/total_step: 55/60 91.67% Loss:0.0053\n","Epoch 23, step/total_step: 60/60 100.00% Loss:0.0034\n","Epoch 23, Val Loss:0.1672\n","Epoch 24, step/total_step: 5/60 8.33% Loss:0.0350\n","Epoch 24, step/total_step: 10/60 16.67% Loss:0.0196\n","Epoch 24, step/total_step: 15/60 25.00% Loss:0.0145\n","Epoch 24, step/total_step: 20/60 33.33% Loss:0.0073\n","Epoch 24, step/total_step: 25/60 41.67% Loss:0.0078\n","Epoch 24, step/total_step: 30/60 50.00% Loss:0.0056\n","Epoch 24, step/total_step: 35/60 58.33% Loss:0.0066\n","Epoch 24, step/total_step: 40/60 66.67% Loss:0.0057\n","Epoch 24, step/total_step: 45/60 75.00% Loss:0.0044\n","Epoch 24, step/total_step: 50/60 83.33% Loss:0.0049\n","Epoch 24, step/total_step: 55/60 91.67% Loss:0.0046\n","Epoch 24, step/total_step: 60/60 100.00% Loss:0.0030\n","Epoch 24, Val Loss:0.1472\n","Epoch 25, step/total_step: 5/60 8.33% Loss:0.0237\n","Epoch 25, step/total_step: 10/60 16.67% Loss:0.0143\n","Epoch 25, step/total_step: 15/60 25.00% Loss:0.0136\n","Epoch 25, step/total_step: 20/60 33.33% Loss:0.0058\n","Epoch 25, step/total_step: 25/60 41.67% Loss:0.0071\n","Epoch 25, step/total_step: 30/60 50.00% Loss:0.0052\n","Epoch 25, step/total_step: 35/60 58.33% Loss:0.0057\n","Epoch 25, step/total_step: 40/60 66.67% Loss:0.0049\n","Epoch 25, step/total_step: 45/60 75.00% Loss:0.0039\n","Epoch 25, step/total_step: 50/60 83.33% Loss:0.0044\n","Epoch 25, step/total_step: 55/60 91.67% Loss:0.0041\n","Epoch 25, step/total_step: 60/60 100.00% Loss:0.0027\n","Epoch 25, Val Loss:0.1497\n","Epoch 26, step/total_step: 5/60 8.33% Loss:0.0199\n","Epoch 26, step/total_step: 10/60 16.67% Loss:0.0117\n","Epoch 26, step/total_step: 15/60 25.00% Loss:0.0104\n","Epoch 26, step/total_step: 20/60 33.33% Loss:0.0062\n","Epoch 26, step/total_step: 25/60 41.67% Loss:0.0057\n","Epoch 26, step/total_step: 30/60 50.00% Loss:0.0042\n","Epoch 26, step/total_step: 35/60 58.33% Loss:0.0053\n","Epoch 26, step/total_step: 40/60 66.67% Loss:0.0045\n","Epoch 26, step/total_step: 45/60 75.00% Loss:0.0033\n","Epoch 26, step/total_step: 50/60 83.33% Loss:0.0038\n","Epoch 26, step/total_step: 55/60 91.67% Loss:0.0037\n","Epoch 26, step/total_step: 60/60 100.00% Loss:0.0024\n","Epoch 26, Val Loss:0.1532\n","Epoch 27, step/total_step: 5/60 8.33% Loss:0.0168\n","Epoch 27, step/total_step: 10/60 16.67% Loss:0.0098\n","Epoch 27, step/total_step: 15/60 25.00% Loss:0.0086\n","Epoch 27, step/total_step: 20/60 33.33% Loss:0.0046\n","Epoch 27, step/total_step: 25/60 41.67% Loss:0.0055\n","Epoch 27, step/total_step: 30/60 50.00% Loss:0.0040\n","Epoch 27, step/total_step: 35/60 58.33% Loss:0.0045\n","Epoch 27, step/total_step: 40/60 66.67% Loss:0.0044\n","Epoch 27, step/total_step: 45/60 75.00% Loss:0.0031\n","Epoch 27, step/total_step: 50/60 83.33% Loss:0.0034\n","Epoch 27, step/total_step: 55/60 91.67% Loss:0.0033\n","Epoch 27, step/total_step: 60/60 100.00% Loss:0.0022\n","Epoch 27, Val Loss:0.1571\n","Epoch 28, step/total_step: 5/60 8.33% Loss:0.0146\n","Epoch 28, step/total_step: 10/60 16.67% Loss:0.0085\n","Epoch 28, step/total_step: 15/60 25.00% Loss:0.0077\n","Epoch 28, step/total_step: 20/60 33.33% Loss:0.0037\n","Epoch 28, step/total_step: 25/60 41.67% Loss:0.0046\n","Epoch 28, step/total_step: 30/60 50.00% Loss:0.0035\n","Epoch 28, step/total_step: 35/60 58.33% Loss:0.0034\n","Epoch 28, step/total_step: 40/60 66.67% Loss:0.0034\n","Epoch 28, step/total_step: 45/60 75.00% Loss:0.0027\n","Epoch 28, step/total_step: 50/60 83.33% Loss:0.0031\n","Epoch 28, step/total_step: 55/60 91.67% Loss:0.0030\n","Epoch 28, step/total_step: 60/60 100.00% Loss:0.0020\n","Epoch 28, Val Loss:0.1522\n","Epoch 29, step/total_step: 5/60 8.33% Loss:0.0125\n","Epoch 29, step/total_step: 10/60 16.67% Loss:0.0072\n","Epoch 29, step/total_step: 15/60 25.00% Loss:0.0067\n","Epoch 29, step/total_step: 20/60 33.33% Loss:0.0032\n","Epoch 29, step/total_step: 25/60 41.67% Loss:0.0038\n","Epoch 29, step/total_step: 30/60 50.00% Loss:0.0029\n","Epoch 29, step/total_step: 35/60 58.33% Loss:0.0031\n","Epoch 29, step/total_step: 40/60 66.67% Loss:0.0029\n","Epoch 29, step/total_step: 45/60 75.00% Loss:0.0024\n","Epoch 29, step/total_step: 50/60 83.33% Loss:0.0027\n","Epoch 29, step/total_step: 55/60 91.67% Loss:0.0027\n","Epoch 29, step/total_step: 60/60 100.00% Loss:0.0018\n","Epoch 29, Val Loss:0.1534\n","Epoch 30, step/total_step: 5/60 8.33% Loss:0.0110\n","Epoch 30, step/total_step: 10/60 16.67% Loss:0.0065\n","Epoch 30, step/total_step: 15/60 25.00% Loss:0.0059\n","Epoch 30, step/total_step: 20/60 33.33% Loss:0.0028\n","Epoch 30, step/total_step: 25/60 41.67% Loss:0.0034\n","Epoch 30, step/total_step: 30/60 50.00% Loss:0.0025\n","Epoch 30, step/total_step: 35/60 58.33% Loss:0.0027\n","Epoch 30, step/total_step: 40/60 66.67% Loss:0.0026\n","Epoch 30, step/total_step: 45/60 75.00% Loss:0.0021\n","Epoch 30, step/total_step: 50/60 83.33% Loss:0.0025\n","Epoch 30, step/total_step: 55/60 91.67% Loss:0.0024\n","Epoch 30, step/total_step: 60/60 100.00% Loss:0.0016\n","Epoch 30, Val Loss:0.1551\n","训练完毕,共用时97秒.\n","评估bilstm模型中...\n"],"name":"stdout"},{"output_type":"stream","text":["/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1236: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"],"name":"stderr"},{"output_type":"stream","text":["           precision    recall  f1-score   support\n","  B-TITLE     0.9229    0.9301    0.9265       772\n","    M-LOC     1.0000    0.8571    0.9231        21\n","   E-RACE     1.0000    1.0000    1.0000        14\n","    E-PRO     0.8857    0.9394    0.9118        33\n","    B-ORG     0.9640    0.9675    0.9657       553\n","    B-LOC     1.0000    0.8333    0.9091         6\n","   E-NAME     1.0000    0.9286    0.9630       112\n","    M-PRO     0.8169    0.8529    0.8345        68\n","    E-EDU     0.9732    0.9732    0.9732       112\n","   E-CONT     1.0000    1.0000    1.0000        28\n","    B-EDU     0.9821    0.9821    0.9821       112\n","   B-NAME     0.9798    0.8661    0.9194       112\n","   M-NAME     0.9268    0.9268    0.9268        82\n","   B-CONT     1.0000    1.0000    1.0000        28\n","  E-TITLE     0.9960    0.9767    0.9863       772\n","    E-ORG     0.9050    0.9132    0.9091       553\n","        O     0.9449    0.9917    0.9678      5190\n","    M-EDU     0.9766    0.9330    0.9543       179\n","   M-CONT     1.0000    1.0000    1.0000        53\n","   B-RACE     1.0000    0.9286    0.9630        14\n","    E-LOC     1.0000    0.8333    0.9091         6\n","    M-ORG     0.9662    0.9582    0.9622      4325\n","    B-PRO     0.8857    0.9394    0.9118        33\n","  M-TITLE     0.9618    0.8658    0.9113      1922\n","avg/total     0.9551    0.9546    0.9542     15100\n","\n","Confusion Matrix:\n","        B-TITLE   M-LOC  E-RACE   E-PRO   B-ORG   B-LOC  E-NAME   M-PRO   E-EDU  E-CONT   B-EDU  B-NAME  M-NAME  B-CONT E-TITLE   E-ORG       O   M-EDU  M-CONT  B-RACE   E-LOC   M-ORG   B-PRO M-TITLE \n","B-TITLE     718       0       0       0       7       0       0       0       0       0       0       0       0       0       0       3      17       0       0       0       0       9       1      17 \n","  M-LOC       0      18       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       3       0       0 \n"," E-RACE       0       0      14       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n","  E-PRO       0       0       0      31       0       0       0       0       0       0       0       0       0       0       0       0       1       1       0       0       0       0       0       0 \n","  B-ORG       3       0       0       0     535       0       0       0       0       0       0       0       0       0       0       0      11       0       0       0       0       4       0       0 \n","  B-LOC       0       0       0       0       1       5       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n"," E-NAME       0       0       0       0       0       0     104       0       0       0       0       0       1       0       0       0       7       0       0       0       0       0       0       0 \n","  M-PRO       0       0       0       0       0       0       0      58       0       0       0       0       0       0       0       2       1       0       0       0       0       6       1       0 \n","  E-EDU       0       0       0       1       0       0       0       0     109       0       0       0       0       0       0       0       1       1       0       0       0       0       0       0 \n"," E-CONT       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n","  B-EDU       0       0       0       1       0       0       0       0       0       0     110       0       0       0       0       0       0       0       0       0       0       1       0       0 \n"," B-NAME       0       0       0       0       0       0       0       0       0       0       0      97       5       0       0       0      10       0       0       0       0       0       0       0 \n"," M-NAME       0       0       0       0       0       0       0       0       0       0       0       2      76       0       0       0       3       0       0       0       0       0       0       0 \n"," B-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0 \n","E-TITLE       0       0       0       0       0       0       0       0       1       0       0       0       0       0     754       0      15       0       0       0       0       0       0       2 \n","  E-ORG       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0     505      17       0       0       0       0      17       1      12 \n","      O       6       0       0       0       2       0       0       1       0       0       0       0       0       0       2       4    5147       0       0       0       0      27       0       1 \n","  M-EDU       0       0       0       0       0       0       0       4       2       0       0       0       0       0       0       1       2     167       0       0       0       1       1       1 \n"," M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0 \n"," B-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0      13       0       0       0       0 \n","  E-LOC       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       5       1       0       0 \n","  M-ORG      14       0       0       0      10       0       0       2       0       0       1       0       0       0       1      19     101       0       0       0       0    4144       0      33 \n","  B-PRO       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       2      31       0 \n","M-TITLE      37       0       0       2       0       0       0       5       0       0       1       0       0       0       0      24     113       2       0       0       0      74       0    1664 \n"],"name":"stdout"}]}]}