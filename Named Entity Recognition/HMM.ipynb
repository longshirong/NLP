{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HMM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPRbHMyEpbsMU7QwVmFPgoU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ojxz3kXC96OZ","colab_type":"code","colab":{}},"source":["import torch\n","\n","\n","class HMM(object):\n","    def __init__(self, N, M):\n","        \"\"\"Args:\n","            N: 状态数，这里对应存在的标注的种类\n","            M: 观测数，这里对应有多少不同的字\n","        \"\"\"\n","        self.N = N\n","        self.M = M\n","\n","        # 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\n","        self.A = torch.zeros(N, N)\n","        # 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\n","        self.B = torch.zeros(N, M)\n","        # 初始状态概率  Pi[i]表示初始时刻为状态i的概率\n","        self.Pi = torch.zeros(N)\n","\n","    def train(self, word_lists, tag_lists, word2id, tag2id):\n","        \"\"\"HMM的训练，即根据训练语料对模型参数进行估计,\n","           因为我们有观测序列以及其对应的状态序列，所以我们\n","           可以使用极大似然估计的方法来估计隐马尔可夫模型的参数\n","        参数:\n","            word_lists: 列表，其中每个元素由字组成的列表，如 ['担','任','科','员']\n","            tag_lists: 列表，其中每个元素是由对应的标注组成的列表，如 ['O','O','B-TITLE', 'E-TITLE']\n","            word2id: 将字映射为ID\n","            tag2id: 字典，将标注映射为ID\n","        \"\"\"\n","\n","        assert len(tag_lists) == len(word_lists)\n","\n","        # 估计状态转移概率矩阵\n","        for tag_list in tag_lists:\n","            seq_len = len(tag_list)\n","            for i in range(seq_len - 1):\n","                current_tagid = tag2id[tag_list[i]]\n","                next_tagid = tag2id[tag_list[i+1]]\n","                self.A[current_tagid][next_tagid] += 1\n","        # 问题：如果某元素没有出现过，该位置为0，这在后续的计算中是不允许的\n","        # 解决方法：我们将等于0的概率加上很小的数\n","        self.A[self.A == 0.] = 1e-10\n","        self.A = self.A / self.A.sum(dim=1, keepdim=True)\n","\n","        #  估计观测概率矩阵，注意没有转移两个字，代码上就可以看出，一个是前后标签的转移\n","        #  正常情况下的标签转移概率会大一些，不是人话的肯定会小一些\n","        #  观测概率矩阵看到的是每个字对应的标签状态，肉眼看到的是字，隐藏的是标签特定顺序对字的顺序的影响，\n","        #  因此矩阵的顺序是标签在为i,字为j\n","        for tag_list, word_list in zip(tag_lists, word_lists):#对标签与字进行打包\n","            assert len(tag_list) == len(word_list)\n","            for tag, word in zip(tag_list, word_list):\n","                tag_id = tag2id[tag]\n","                word_id = word2id[word]\n","                self.B[tag_id][word_id] += 1\n","        self.B[self.B == 0.] = 1e-10\n","        self.B = self.B / self.B.sum(dim=1, keepdim=True)\n","\n","        # 估计初始状态概率，tag_list[0]表示一句话的开头，这样的不同的标签的在整个可能下的初始概率\n","        for tag_list in tag_lists:\n","            init_tagid = tag2id[tag_list[0]]\n","            self.Pi[init_tagid] += 1\n","        self.Pi[self.Pi == 0.] = 1e-10\n","        self.Pi = self.Pi / self.Pi.sum()\n","    #将字通过训练后预测对应的标签\n","    def test(self, word_lists, word2id, tag2id):\n","        pred_tag_lists = []\n","        for word_list in word_lists:\n","            pred_tag_list = self.decoding(word_list, word2id, tag2id)\n","            pred_tag_lists.append(pred_tag_list)\n","        return pred_tag_lists\n","\n","    def decoding(self, word_list, word2id, tag2id):\n","        \"\"\"\n","        使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。\n","        维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）\n","        这时一条路径对应着一个状态序列\n","        \"\"\"\n","        # 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢\n","        # 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数\n","        #  同时相乘操作也变成简单的相加操作\n","        A = torch.log(self.A)\n","        B = torch.log(self.B)\n","        Pi = torch.log(self.Pi)\n","\n","        # 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]\n","        # 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值，N是状态数\n","        seq_len = len(word_list)\n","        viterbi = torch.zeros(self.N, seq_len)\n","        # backpointer是跟viterbi一样大小的矩阵\n","        # backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id\n","        # 等解码的时候，我们用backpointer进行回溯，以求出最优路径\n","        backpointer = torch.zeros(self.N, seq_len).long()\n","\n","        # self.Pi[i] 表示第一个字的标记为i的概率\n","        # Bt[word_id]表示字为word_id的时候，对应各个标记的概率\n","        # self.A.t()[tag_id]表示各个状态转移到tag_id对应的概率\n","\n","        # 所以第一步为\n","        start_wordid = word2id.get(word_list[0], None)\n","        Bt = B.t()\n","        if start_wordid is None:\n","            # 如果字不再字典里，则假设状态的概率分布是均匀的\n","            bt = torch.log(torch.ones(self.N) / self.N)\n","        else:\n","            bt = Bt[start_wordid]\n","        viterbi[:, 0] = Pi + bt\n","        backpointer[:, 0] = -1\n","\n","        # 递推公式：\n","        # viterbi[tag_id, step] = max(viterbi[:, step-1]* self.A.t()[tag_id] * Bt[word])\n","        # 其中word是step时刻对应的字\n","        # 由上述递推公式求后续各步\n","        for step in range(1, seq_len):\n","            wordid = word2id.get(word_list[step], None)\n","            # 处理字不在字典中的情况\n","            # bt是在t时刻字为wordid时，状态的概率分布\n","            if wordid is None:\n","                # 如果字不再字典里，则假设状态的概率分布是均匀的\n","                bt = torch.log(torch.ones(self.N) / self.N)\n","            else:\n","                bt = Bt[wordid]  # 否则从观测概率矩阵中取bt\n","            for tag_id in range(len(tag2id)):\n","                max_prob, max_id = torch.max(\n","                    viterbi[:, step-1] + A[:, tag_id],\n","                    dim=0\n","                )\n","                viterbi[tag_id, step] = max_prob + bt[tag_id]\n","                backpointer[tag_id, step] = max_id\n","\n","        # 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率\n","        best_path_prob, best_path_pointer = torch.max(\n","            viterbi[:, seq_len-1], dim=0\n","        )\n","\n","        # 回溯，求最优路径\n","        best_path_pointer = best_path_pointer.item()\n","        best_path = [best_path_pointer]\n","        for back_step in range(seq_len-1, 0, -1):\n","            best_path_pointer = backpointer[best_path_pointer, back_step]\n","            best_path_pointer = best_path_pointer.item()\n","            best_path.append(best_path_pointer)\n","\n","        # 将tag_id组成的序列转化为tag\n","        assert len(best_path) == len(word_list)\n","        id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\n","        tag_list = [id2tag[id_] for id_ in reversed(best_path)]\n","\n","        return tag_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuD0Tu0k_kst","colab_type":"code","outputId":"636094f2-321d-4000-d08b-561137461109","executionInfo":{"status":"ok","timestamp":1585228899932,"user_tz":-480,"elapsed":54127,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P0JEaVYTAF8N","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"THdhLofb-Dar","colab_type":"code","colab":{}},"source":["from os.path import join\n","from codecs import open\n","\n","\n","def build_corpus(split, make_vocab=True, data_dir=\"/content/gdrive/My Drive/ML/NLP/传统NLP/data\"):\n","    \"\"\"读取数据\"\"\"\n","    assert split in ['train', 'dev', 'test']\n","\n","    word_lists = []\n","    tag_lists = []\n","    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n","        word_list = []\n","        tag_list = []\n","        for line in f:\n","            if line != '\\n':   #表示这一行有字和标签\n","                word, tag = line.strip('\\n').split()    #将字和标签分隔开\n","                word_list.append(word)\n","                tag_list.append(tag)\n","            else:\n","                word_lists.append(word_list) #将临时保存的句子和标签序列加入到字序列以及标签序列里，并清零\n","                tag_lists.append(tag_list)\n","                word_list = []\n","                tag_list = []\n","\n","    # 如果make_vocab为True，还需要返回word2id和tag2id\n","    if make_vocab:\n","        word2id = build_map(word_lists)\n","        tag2id = build_map(tag_lists)\n","        return word_lists, tag_lists, word2id, tag2id\n","    else:\n","        return word_lists, tag_lists\n","\n","\n","def build_map(lists):\n","    maps = {}\n","    for list_ in lists: #lists是很多条序列，list_是一条序列\n","        for e in list_:  #取出里面的一个字或者一个标签\n","            if e not in maps:\n","                maps[e] = len(maps)  #字或者标签的id是准不递增的，一开始是0，然后有了元素之后len(maps)为1，依次类推\n","\n","    return maps\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPyQZekS-QrM","colab_type":"code","colab":{}},"source":["import pickle\n","\n","\n","def merge_maps(dict1, dict2):\n","    \"\"\"用于合并两个word2id或者两个tag2id\"\"\"\n","    for key in dict2.keys():\n","        if key not in dict1:\n","            dict1[key] = len(dict1)\n","    return dict1\n","\n","\n","def save_model(model, file_name):\n","    \"\"\"用于保存模型\"\"\"\n","    with open(file_name, \"wb\") as f:\n","        pickle.dump(model, f)\n","\n","\n","def load_model(file_name):\n","    \"\"\"用于加载模型\"\"\"\n","    with open(file_name, \"rb\") as f:\n","        model = pickle.load(f)\n","    return model\n","\n","\n","\n","def prepocess_data_for_lstmcrf(word_lists, tag_lists, test=False):\n","    assert len(word_lists) == len(tag_lists)\n","    for i in range(len(word_lists)):\n","        word_lists[i].append(\"<end>\")\n","        if not test:  # 如果是测试数据，就不需要加end token了\n","            tag_lists[i].append(\"<end>\")\n","\n","    return word_lists, tag_lists\n","\n","\n","def flatten_lists(lists):\n","    flatten_list = []\n","    for l in lists:\n","        if type(l) == list:\n","            flatten_list += l\n","        else:\n","            flatten_list.append(l)\n","    return flatten_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1SggbeZCz35","colab_type":"code","colab":{}},"source":["from collections import Counter\n","\n","\n","\n","class Metrics(object):\n","    \"\"\"用于评价模型，计算每个标签的精确率，召回率，F1分数\"\"\"\n","\n","    def __init__(self, golden_tags, predict_tags, remove_O=False):\n","\n","        # [[t1, t2], [t3, t4]...] --> [t1, t2, t3, t4...]\n","        self.golden_tags = flatten_lists(golden_tags)\n","        self.predict_tags = flatten_lists(predict_tags)\n","\n","        if remove_O:  # 将O标记移除，只关心实体标记\n","            self._remove_Otags()\n","\n","        # 辅助计算的变量\n","        self.tagset = set(self.golden_tags)\n","        self.correct_tags_number = self.count_correct_tags()\n","        self.predict_tags_counter = Counter(self.predict_tags)\n","        self.golden_tags_counter = Counter(self.golden_tags)\n","\n","        # 计算精确率\n","        self.precision_scores = self.cal_precision()\n","\n","        # 计算召回率\n","        self.recall_scores = self.cal_recall()\n","\n","        # 计算F1分数\n","        self.f1_scores = self.cal_f1()\n","\n","    def cal_precision(self):\n","\n","        precision_scores = {}\n","        for tag in self.tagset:\n","            precision_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n","                self.predict_tags_counter[tag]\n","\n","        return precision_scores\n","\n","    def cal_recall(self):\n","\n","        recall_scores = {}\n","        for tag in self.tagset:\n","            recall_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n","                self.golden_tags_counter[tag]\n","        return recall_scores\n","\n","    def cal_f1(self):\n","        f1_scores = {}\n","        for tag in self.tagset:\n","            p, r = self.precision_scores[tag], self.recall_scores[tag]\n","            f1_scores[tag] = 2*p*r / (p+r+1e-10)  # 加上一个特别小的数，防止分母为0\n","        return f1_scores\n","\n","    def report_scores(self):\n","        \"\"\"将结果用表格的形式打印出来，像这个样子：\n","\n","                      precision    recall  f1-score   support\n","              B-LOC      0.775     0.757     0.766      1084\n","              I-LOC      0.601     0.631     0.616       325\n","             B-MISC      0.698     0.499     0.582       339\n","             I-MISC      0.644     0.567     0.603       557\n","              B-ORG      0.795     0.801     0.798      1400\n","              I-ORG      0.831     0.773     0.801      1104\n","              B-PER      0.812     0.876     0.843       735\n","              I-PER      0.873     0.931     0.901       634\n","\n","          avg/total      0.779     0.764     0.770      6178\n","        \"\"\"\n","        # 打印表头\n","        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n","        header = ['precision', 'recall', 'f1-score', 'support']\n","        print(header_format.format('', *header))\n","\n","        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n","        # 打印每个标签的 精确率、召回率、f1分数\n","        for tag in self.tagset:\n","            print(row_format.format(\n","                tag,\n","                self.precision_scores[tag],\n","                self.recall_scores[tag],\n","                self.f1_scores[tag],\n","                self.golden_tags_counter[tag]\n","            ))\n","\n","        # 计算并打印平均值\n","        avg_metrics = self._cal_weighted_average()\n","        print(row_format.format(\n","            'avg/total',\n","            avg_metrics['precision'],\n","            avg_metrics['recall'],\n","            avg_metrics['f1_score'],\n","            len(self.golden_tags)\n","        ))\n","\n","    def count_correct_tags(self):\n","        \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n","        correct_dict = {}\n","        for gold_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n","            if gold_tag == predict_tag:\n","                if gold_tag not in correct_dict:\n","                    correct_dict[gold_tag] = 1\n","                else:\n","                    correct_dict[gold_tag] += 1\n","\n","        return correct_dict\n","\n","    def _cal_weighted_average(self):\n","\n","        weighted_average = {}\n","        total = len(self.golden_tags)\n","\n","        # 计算weighted precisions:\n","        weighted_average['precision'] = 0.\n","        weighted_average['recall'] = 0.\n","        weighted_average['f1_score'] = 0.\n","        for tag in self.tagset:\n","            size = self.golden_tags_counter[tag]\n","            weighted_average['precision'] += self.precision_scores[tag] * size\n","            weighted_average['recall'] += self.recall_scores[tag] * size\n","            weighted_average['f1_score'] += self.f1_scores[tag] * size\n","\n","        for metric in weighted_average.keys():\n","            weighted_average[metric] /= total\n","\n","        return weighted_average\n","\n","    def _remove_Otags(self):\n","\n","        length = len(self.golden_tags)\n","        O_tag_indices = [i for i in range(length)\n","                         if self.golden_tags[i] == 'O']\n","\n","        self.golden_tags = [tag for i, tag in enumerate(self.golden_tags)\n","                            if i not in O_tag_indices]\n","\n","        self.predict_tags = [tag for i, tag in enumerate(self.predict_tags)\n","                             if i not in O_tag_indices]\n","        print(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n","            length,\n","            len(O_tag_indices),\n","            len(O_tag_indices) / length * 100\n","        ))\n","\n","    def report_confusion_matrix(self):\n","        \"\"\"计算混淆矩阵\"\"\"\n","\n","        print(\"\\nConfusion Matrix:\")\n","        tag_list = list(self.tagset)\n","        # 初始化混淆矩阵 matrix[i][j]表示第i个tag被模型预测成第j个tag的次数\n","        tags_size = len(tag_list)\n","        matrix = []\n","        for i in range(tags_size):\n","            matrix.append([0] * tags_size)\n","\n","        # 遍历tags列表\n","        for golden_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n","            try:\n","                row = tag_list.index(golden_tag)\n","                col = tag_list.index(predict_tag)\n","                matrix[row][col] += 1\n","            except ValueError:  # 有极少数标记没有出现在golden_tags，但出现在predict_tags，跳过这些标记\n","                continue\n","\n","        # 输出矩阵\n","        row_format_ = '{:>7} ' * (tags_size+1)\n","        print(row_format_.format(\"\", *tag_list))\n","        for i, row in enumerate(matrix):\n","            print(row_format_.format(tag_list[i], *row))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HX9893u-apw","colab_type":"code","colab":{}},"source":["def hmm_train_eval(train_data, test_data, word2id, tag2id, remove_O=False):\n","    \"\"\"训练并评估hmm模型\"\"\"\n","    # 训练HMM模型\n","    train_word_lists, train_tag_lists = train_data\n","    test_word_lists, test_tag_lists = test_data\n","\n","    hmm_model = HMM(len(tag2id), len(word2id))\n","    hmm_model.train(train_word_lists,\n","                    train_tag_lists,\n","                    word2id,\n","                    tag2id)\n","    save_model(hmm_model, \"/content/gdrive/My Drive/ML/NLP/传统NLP/data/hmm.pkl\")\n","\n","    # 评估hmm模型\n","    pred_tag_lists = hmm_model.test(test_word_lists,\n","                                    word2id,\n","                                    tag2id)\n","\n","    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=remove_O)\n","    metrics.report_scores()\n","    metrics.report_confusion_matrix()\n","\n","    return pred_tag_lists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"byKT5Nam-gvo","colab_type":"code","outputId":"ca285cc3-ce56-48e2-987b-a34d912ccd8a","executionInfo":{"status":"ok","timestamp":1585229640015,"user_tz":-480,"elapsed":20439,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def main():\n","    \"\"\"训练模型，评估结果\"\"\"\n","\n","    # 读取数据\n","    print(\"读取数据...\")\n","    train_word_lists, train_tag_lists, word2id, tag2id = \\\n","        build_corpus(\"train\")\n","    dev_word_lists, dev_tag_lists = build_corpus(\"dev\", make_vocab=False)\n","    test_word_lists, test_tag_lists = build_corpus(\"test\", make_vocab=False)\n","\n","    # 训练评估ｈｍｍ模型\n","    print(\"正在训练评估HMM模型...\")\n","    hmm_pred = hmm_train_eval(\n","        (train_word_lists, train_tag_lists),\n","        (test_word_lists, test_tag_lists),\n","        word2id,\n","        tag2id\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["读取数据...\n","正在训练评估HMM模型...\n","           precision    recall  f1-score   support\n","    E-LOC     0.5000    0.5000    0.5000         6\n","    M-LOC     0.5833    0.3333    0.4242        21\n","   B-CONT     0.9655    1.0000    0.9825        28\n","    E-EDU     0.9167    0.9821    0.9483       112\n","   E-CONT     0.9655    1.0000    0.9825        28\n","    M-EDU     0.9348    0.9609    0.9477       179\n","    B-LOC     0.3333    0.3333    0.3333         6\n","    B-PRO     0.5581    0.7273    0.6316        33\n","   M-NAME     0.9459    0.8537    0.8974        82\n","    E-PRO     0.6512    0.8485    0.7368        33\n","   E-RACE     1.0000    0.9286    0.9630        14\n","  E-TITLE     0.9514    0.9637    0.9575       772\n","    M-PRO     0.4490    0.6471    0.5301        68\n","        O     0.9568    0.9177    0.9369      5190\n","  B-TITLE     0.8811    0.8925    0.8867       772\n","    B-ORG     0.8422    0.8879    0.8644       553\n","   B-RACE     1.0000    0.9286    0.9630        14\n","  M-TITLE     0.9038    0.8751    0.8892      1922\n","   E-NAME     0.9000    0.8036    0.8491       112\n","    E-ORG     0.8262    0.8680    0.8466       553\n","   B-NAME     0.9800    0.8750    0.9245       112\n","    B-EDU     0.9000    0.9643    0.9310       112\n","   M-CONT     0.9815    1.0000    0.9907        53\n","    M-ORG     0.9002    0.9327    0.9162      4325\n","avg/total     0.9149    0.9122    0.9130     15100\n","\n","Confusion Matrix:\n","          E-LOC   M-LOC  B-CONT   E-EDU  E-CONT   M-EDU   B-LOC   B-PRO  M-NAME   E-PRO  E-RACE E-TITLE   M-PRO       O B-TITLE   B-ORG  B-RACE M-TITLE  E-NAME   E-ORG  B-NAME   B-EDU  M-CONT   M-ORG \n","  E-LOC       3       0       0       0       0       0       0       0       0       0       0       0       0       2       0       0       0       0       0       0       0       0       0       1 \n","  M-LOC       0       7       0       0       0       0       1       0       0       0       0       0       0       4       0       0       0       0       0       2       0       0       0       7 \n"," B-CONT       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n","  E-EDU       0       0       0     110       0       1       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n"," E-CONT       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n","  M-EDU       0       0       0       0       0     172       0       1       0       0       0       0       4       1       0       0       0       0       0       1       0       0       0       0 \n","  B-LOC       0       0       0       0       0       0       2       0       0       0       0       0       0       1       0       3       0       0       0       0       0       0       0       0 \n","  B-PRO       0       0       0       0       0       0       0      24       0       0       0       0       3       0       0       0       0       0       0       0       0       1       0       5 \n"," M-NAME       0       0       0       0       0       0       0       0      70       0       0       0       0       3       0       0       0       0       6       0       0       0       0       3 \n","  E-PRO       0       0       0       1       0       1       0       0       0      28       0       0       0       0       0       0       0       0       0       2       0       1       0       0 \n"," E-RACE       0       0       0       0       0       0       0       0       0       0      13       0       0       1       0       0       0       0       0       0       0       0       0       0 \n","E-TITLE       0       0       0       1       0       0       0       0       0       0       0     744       0       6       0       4       0       2       0       0       0       0       0      15 \n","  M-PRO       0       0       0       0       0       1       0       1       0       0       0       0      44       0       0       0       0       0       0       3       0       1       0      18 \n","      O       0       0       0       2       0       1       0       3       0       4       0      26      12    4763      26      37       0      78       2      30       0       1       0     204 \n","B-TITLE       0       0       0       0       0       0       0       2       0       0       0       1       0      20     689       6       0      28       0       1       0       2       0      23 \n","  B-ORG       0       0       1       0       0       0       3       0       0       0       0       0       0      28       6     491       0       0       0       0       1       0       0      23 \n"," B-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0      13       0       0       0       0       0       0       0 \n","M-TITLE       0       0       0       3       0       4       0       1       0       3       0       6       7      44      35       3       0    1682       0      17       0       2       0     115 \n"," E-NAME       0       0       0       0       0       0       0       0       2       0       0       0       0      16       0       0       0       0      90       3       0       0       0       0 \n","  E-ORG       0       0       0       0       0       0       0       1       0       0       0       1       3      10       9       0       0      18       0     480       0       1       0      30 \n"," B-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0       8       0       1       0       0       0       0      98       0       0       2 \n","  B-EDU       0       0       0       0       0       3       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0     108       0       1 \n"," M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0 \n","  M-ORG       3       5       0       3       1       1       0      10       2       7       0       4      25      70      17      38       0      53       2      42       1       3       1    4034 \n"],"name":"stdout"}]}]}