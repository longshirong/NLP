{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"bilstm.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ocxjDdyQJrxv","colab_type":"code","outputId":"ea86c715-e961-4460-b895-32f77fcf015f","executionInfo":{"status":"ok","timestamp":1586845033762,"user_tz":-480,"elapsed":3195,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# -*- coding: utf-8 -*-\n","#双向LSTM\n","from keras.layers import Input, Dense, Embedding, LSTM, Dropout, TimeDistributed, Bidirectional\n","from keras.models import Model, load_model\n","from keras.utils import np_utils\n","import numpy as np\n","import re"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-jIsCMulGs5l","colab_type":"code","outputId":"8d912d1f-1cfa-41e7-88e9-44c062c26d5c","executionInfo":{"status":"ok","timestamp":1586845063723,"user_tz":-480,"elapsed":28236,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sxZcvFM5G4wP","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/gdrive/My Drive/NLP/Chinese_Word_Segmentation')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbvXJMg4Jrx2","colab_type":"code","outputId":"3d6458d9-8170-4f2a-b121-4e4b85481f6b","executionInfo":{"status":"ok","timestamp":1586845073846,"user_tz":-480,"elapsed":1904,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 读取字典\n","vocab = open('data/msr/msr_training_words.utf8').read().rstrip('\\n').split('\\n')#读取词表\n","vocab = list(''.join(vocab))#提取词表里的字\n","stat = {}\n","for v in vocab:\n","    stat[v] = stat.get(v, 0) + 1\n","stat = sorted(stat.items(), key=lambda x:x[1], reverse=True)\n","vocab = [s[0] for s in stat]\n","# 5167 个字\n","print(len(vocab))\n","# 映射\n","char2id = {c : i + 1 for i, c in enumerate(vocab)}\n","id2char = {i + 1 : c for i, c in enumerate(vocab)}\n","tags = {'s': 0, 'b': 1, 'm': 2, 'e': 3, 'x': 4}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5167\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t4nNRbtvJrx7","colab_type":"code","colab":{}},"source":["embedding_size = 128\n","maxlen = 32 # 长于32则截断，短于32则填充0\n","hidden_size = 64\n","batch_size = 64\n","epochs = 50"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHEqW1BJJryA","colab_type":"code","outputId":"ba47c68c-0ffb-4d31-a678-fc280cb61a45","executionInfo":{"status":"ok","timestamp":1586845101462,"user_tz":-480,"elapsed":11683,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["def load_data(path):\n","    data = open(path).read().rstrip('\\n')\n","    # 按标点符号和换行符分隔\n","    data = re.split('[，。！？、\\n]', data)\n","    print('共有数据 %d 条' % len(data))\n","    print('平均长度：', np.mean([len(d.replace(' ', '')) for d in data]))\t#replace去掉空格，这里得到的就是原始的未分词过的句子了，同理测试集也是如此\n","    \n","    # 准备数据\n","    X_data = []\n","    y_data = []\n","    \n","    for sentence in data:\n","        sentence = sentence.split(' ')\n","        X = []\n","        y = []\n","        \n","        try:\n","            for s in sentence:\n","                s = s.strip()\n","                # 跳过空字符\n","                if len(s) == 0:\n","                    continue\n","                # s\n","                elif len(s) == 1:\n","                    X.append(char2id[s])\n","                    y.append(tags['s'])\n","                elif len(s) > 1:\n","                    # b\n","                    X.append(char2id[s[0]])\n","                    y.append(tags['b'])\n","                    # m\n","                    for i in range(1, len(s) - 1):\n","                        X.append(char2id[s[i]])\n","                        y.append(tags['m'])\n","                    # e\n","                    X.append(char2id[s[-1]])\n","                    y.append(tags['e'])\n","            \n","            # 统一长度\n","            if len(X) > maxlen:\n","                X = X[:maxlen]\n","                y = y[:maxlen]\n","            else:\n","                for i in range(maxlen - len(X)):\n","                    X.append(0)\n","                    y.append(tags['x'])#填充的状态，所以最终有5个状态\n","        except:\n","            continue\n","        else:\n","            if len(X) > 0:\n","                X_data.append(X)\n","                y_data.append(y)\n","    \n","    X_data = np.array(X_data)\n","    y_data = np_utils.to_categorical(y_data, 5)#标签总类为5的独热编码，也就是一个句子里的每个字的标签\n","    \n","    return X_data, y_data\n","\n","X_train, y_train = load_data('data/msr/msr_training.utf8')#已经将句子分词好的训练词表\n","X_test, y_test = load_data('data/msr/msr_test_gold.utf8')#国际标准的分词结果\n","print('X_train size:', X_train.shape)\n","print('y_train size:', y_train.shape)\n","print('X_test size:', X_test.shape)\n","print('y_test size:', y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["共有数据 385152 条\n","平均长度： 9.742236831173146\n","共有数据 17961 条\n","平均长度： 9.48605311508268\n","X_train size: (385152, 32)\n","y_train size: (385152, 32, 5)\n","X_test size: (17917, 32)\n","y_test size: (17917, 32, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d4ZVjYJuVtA-","colab_type":"code","outputId":"c6eac469-ecff-4eeb-cee7-88a465821189","executionInfo":{"status":"ok","timestamp":1586845121386,"user_tz":-480,"elapsed":856,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["X_train[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 338,   29, 2038,  333,  496,   78,  473,  605,   23,   33,  173,\n","         68,  222,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"3UK8vdt_VxNM","colab_type":"code","outputId":"1f45d6a1-6060-4eb2-e986-787af930c9aa","executionInfo":{"status":"ok","timestamp":1586845194944,"user_tz":-480,"elapsed":1270,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_train[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 5)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"k6e2X0y2JryF","colab_type":"code","outputId":"434ec43b-df41-444e-cc4f-3d7d713fe940","executionInfo":{"status":"ok","timestamp":1586847672821,"user_tz":-480,"elapsed":1110562,"user":{"displayName":"shirong long","photoUrl":"","userId":"01201328390703623656"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["#对于LSTM以及BiLSTM可以参考https://www.jiqizhixin.com/articles/2018-10-24-13，实际上就是两个LSTM使用，一个前向一个后向\n","X = Input(shape=(maxlen,), dtype='int32')\n","#填补的0值在后续的计算中不产生影响, 就可以在初始化Embedding层时指定参数mask_zero为True, 意思就是屏蔽0值, 即填补的0值.\n","embedding = Embedding(input_dim=len(vocab) + 1, output_dim=embedding_size, input_length=maxlen, mask_zero=True)(X)\n","#返回全部hidden step，合并模式，也即是将两个LSTM连接起来，hidden_size = 64，隐层状态越多描述越精确但是会越耗费时间\n","blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(embedding)\n","blstm = Dropout(0.6)(blstm)\n","#hidden_size 隐层状态的维数：（每个LSTM单元或者时间步的输出的ht的维度，单元内部有权重与偏差计算）\n","blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(blstm)\n","blstm = Dropout(0.6)(blstm)\n","#最后对应的由五种状态\n","output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n","\n","model = Model(X, output)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n","model.save('msr_bilstm.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","385152/385152 [==============================] - 1106s 3ms/step - loss: 0.1254 - accuracy: 0.8487\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7fsjFDemJryJ","colab_type":"code","outputId":"30eb504b-0c06-4a27-999e-eb4f27ba845c","colab":{}},"source":["print(model.evaluate(X_train, y_train, batch_size=batch_size))\n","print(model.evaluate(X_test, y_test, batch_size=batch_size))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["385152/385152 [==============================] - 438s 1ms/step\n","[0.033360598500879995, 0.9891092663648795]\n","17917/17917 [==============================] - 21s 1ms/step\n","[0.18042443292314747, 0.9547670693238373]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"645GN9x-JryN","colab_type":"code","colab":{}},"source":["def viterbi(nodes):\n","    trans = {'be': 0.5, 'bm': 0.5, 'eb': 0.5, 'es': 0.5, 'me': 0.5, 'mm': 0.5, 'sb': 0.5, 'ss': 0.5}\n","    paths = {'b': nodes[0]['b'], 's': nodes[0]['s']}\n","    for l in range(1, len(nodes)):\n","        paths_ = paths.copy()\n","        paths = {}\n","        for i in nodes[l].keys():\n","            nows = {}\n","            for j in paths_.keys():\n","                if j[-1] + i in trans.keys():\n","                    nows[j + i] = paths_[j] + nodes[l][i] + trans[j[-1] + i]\n","            nows = sorted(nows.items(), key=lambda x: x[1], reverse=True)\n","            paths[nows[0][0]] = nows[0][1]\n","    \n","    paths = sorted(paths.items(), key=lambda x: x[1], reverse=True)\n","    return paths[0][0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Yp3N0sFJryU","colab_type":"code","colab":{}},"source":["def cut_words(data):\n","    data = re.split('[，。！？、\\n]', data)\n","    sens = []\n","    Xs = []\n","    for sentence in data:\n","        sen = []\n","        X = []\n","        sentence = list(sentence)\n","        for s in sentence:\n","            s = s.strip()\n","            if not s == '' and s in char2id:\n","                sen.append(s)\n","                X.append(char2id[s])\n","        if len(X) > maxlen:\n","            sen = sen[:maxlen]\n","            X = X[:maxlen]\n","        else:\n","            for i in range(maxlen - len(X)):\n","                X.append(0)\n","        \n","        if len(sen) > 0:\n","            Xs.append(X)\n","            sens.append(sen)\n","    \n","    Xs = np.array(Xs)\n","    ys = model.predict(Xs)\n","    \n","    results = ''\n","    for i in range(ys.shape[0]):\n","        nodes = [dict(zip(['s', 'b', 'm', 'e'], d[:4])) for d in ys[i]]\n","        ts = viterbi(nodes)\n","        for x in range(len(sens[i])):\n","            if ts[x] in ['s', 'e']:\n","                results += sens[i][x] + '/'\n","            else:\n","                results += sens[i][x]\n","        \n","    return results[:-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiDk0YhBJryZ","colab_type":"code","outputId":"a010b60d-6b5e-49ef-ea3f-0f3ef0579245","colab":{}},"source":["print(cut_words('中国共产党第十九次全国代表大会，是在全面建成小康社会决胜阶段、中国特色社会主义进入新时代的关键时期召开的一次十分重要的大会。'))\n","print(cut_words('把这本书推荐给，具有一定编程基础，希望了解数据分析、人工智能等知识领域，进一步提升个人技术能力的社会各界人士。'))\n","print(cut_words('结婚的和尚未结婚的。'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["中国共产党第十九次全国代表大会/是/在/全面/建成/小康/社会/决胜/阶段/中国/特色/社会主义/进入/新时代/的/关键/时期/召开/的/一次/十分/重要/的/大会\n","把/这/本/书/推荐/给/具有/一定/编程/基础/希望/了解/数据/分析/人工/智能/等/知识/领域/进一步/提升/个人/技术/能力/的/社会/各界人士\n","结婚/的/和/尚未/结婚/的\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b-9bOjH-Jryd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}